{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMbcYyIakVSKiqPG6OZ57fu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AugusGuarna/Feedforward_NN/blob/main/NN_multiclass_pred.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural network for multiclass prediction\n",
        "\n",
        "The purpose of this notebook is to implement a neural network for multiclass prediction using the Iris dataset and PyTorch.\n",
        "\n",
        "## Defining an arquitecture\n",
        "\n",
        "First of all we're going to define the arquitecture of the whole NN.\n",
        "\n",
        "\n",
        "*   Input layer: a vector living in $\\mathbb{R}^{4}$.\n",
        "*   Hidden layers: 3 hidden layers with 6, 8 and 10 neurons each.\n",
        "*   Output layer: a vector living in $\\mathbb{R}^{3}$.\n",
        "*   Activation functions: for the hidden layers we're going to use ReLu and for the ouput layer the Softmax.\n",
        "\n",
        "\n",
        "The reason behind the dimensions of the input and output layer rely on the fact that each of the entries in the dataset counts with 4 attributes and we want to classify according to 3 classes: `setosa`, `versicolor` and `virginica`."
      ],
      "metadata": {
        "id": "Fg8w63XDJlZ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Libraries\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset,TensorDataset, DataLoader, random_split\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.datasets import load_iris\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "uflcON-HxJ1O"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_iris()\n",
        "\n",
        "# We split the data and the target\n",
        "X = dataset[\"data\"]\n",
        "y = dataset[\"target\"]\n",
        "y = LabelEncoder().fit_transform(y)\n",
        "\n",
        "full_dataset = TensorDataset(torch.from_numpy(X).float(), torch.from_numpy(y))\n",
        "\n",
        "# As we don't have much data we're going to perform an 0.9-0.1 train-test split\n",
        "train_size = int(0.9 * len(full_dataset))\n",
        "test_size = len(full_dataset) - train_size\n",
        "train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
        "\n",
        "# Now we transform the datasets into dataloaders\n",
        "batch_size = 5\n",
        "\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    dataset=test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")\n"
      ],
      "metadata": {
        "id": "7wd_JnzutbvQ"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "XbRqFdOjJF2s"
      },
      "outputs": [],
      "source": [
        "class FeedForwardNeuralNetwork(nn.Module):\n",
        "  def __init__(self, input_size, output_size, number_layers, size_layers):\n",
        "    super().__init__()\n",
        "    self.weights = nn.ParameterList()\n",
        "    self.bias = nn.ParameterList()\n",
        "    self.number_layers = number_layers\n",
        "    self.size_layers = size_layers\n",
        "    for i in range(number_layers+1):\n",
        "      if i == 0:\n",
        "        self.weights.append(nn.Parameter(torch.randn(size=(input_size, size_layers[i]),\n",
        "                                                         dtype=torch.float, requires_grad=True)))\n",
        "        self.bias.append(nn.Parameter(torch.randn(size_layers[i]), requires_grad=True))\n",
        "      elif i > 0 and i < number_layers:\n",
        "        self.weights.append(nn.Parameter(torch.randn(size=(size_layers[i-1], size_layers[i]),\n",
        "                                                         dtype=torch.float, requires_grad=True)))\n",
        "        self.bias.append(nn.Parameter(torch.randn(size_layers[i]), requires_grad=True))\n",
        "      else:\n",
        "        self.weights.append(nn.Parameter(torch.randn(size=(size_layers[i-1], output_size),\n",
        "                                                         dtype=torch.float, requires_grad=True)))\n",
        "        self.bias.append(nn.Parameter(torch.randn(output_size), requires_grad=True))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, data):\n",
        "    Z = data\n",
        "    for i in range(len(self.weights)):\n",
        "      Z = torch.matmul(Z, self.weights[i]) + self.bias[i]\n",
        "      if i < len(self.weights)-1:\n",
        "        Z = F.relu(Z)\n",
        "    return Z"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "model0 = FeedForwardNeuralNetwork(4,3,3,[6,8,10])\n"
      ],
      "metadata": {
        "id": "0XmrOang5BXE"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We check wether everything is ok\n",
        "model0.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXv796Ia5ePE",
        "outputId": "559decb8-8045-4d8e-8db0-5c2bbf1c2074"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('weights.0',\n",
              "              tensor([[ 1.9269,  1.4873,  0.9007, -2.1055,  0.6784, -1.2345],\n",
              "                      [-0.0431, -1.6047,  0.3559, -0.6866, -0.4934,  0.2415],\n",
              "                      [-1.1109,  0.0915, -2.3169, -0.2168, -0.3097, -0.3957],\n",
              "                      [ 0.8034, -0.6216, -0.5920, -0.0631, -0.8286,  0.3309]])),\n",
              "             ('weights.1',\n",
              "              tensor([[-0.4880,  1.1914, -0.8140, -0.7360, -0.8371, -0.9224,  1.8113,  0.1606],\n",
              "                      [-0.0978,  1.8446, -1.1845,  1.3835, -1.2024,  0.7078, -1.0759,  0.5357],\n",
              "                      [ 0.3466, -0.1973, -1.0546,  1.2780,  0.1453,  0.2311,  0.0087, -0.1423],\n",
              "                      [ 0.5750, -0.6417, -2.2064, -0.7508,  2.8140,  0.3598, -0.0898,  0.4584],\n",
              "                      [ 0.5362,  0.5246,  1.1412,  0.0516,  0.7281, -0.7106, -0.6021,  0.9604],\n",
              "                      [-1.7223, -0.8278,  1.3347,  0.4835, -0.1976,  1.2683,  1.2243,  0.0981]])),\n",
              "             ('weights.2',\n",
              "              tensor([[ 0.7262,  0.0912, -0.3891,  0.5279,  1.0311, -0.7048,  1.0131, -0.3308,\n",
              "                        1.0950,  0.3399],\n",
              "                      [ 0.7200,  0.4114, -0.5733,  0.5069, -0.4752, -0.4920, -0.1360,  1.6354,\n",
              "                        0.6547,  0.5760],\n",
              "                      [-0.3609, -0.0606,  0.0733,  0.8187, -0.3753,  1.0331, -0.6867,  0.6368,\n",
              "                        0.2176, -0.0467],\n",
              "                      [-1.4335, -0.5665,  0.2695, -0.2104, -0.7328,  0.1043,  1.0414, -0.3997,\n",
              "                       -2.2933,  0.4976],\n",
              "                      [-2.4801, -0.4175, -1.1955,  0.8123, -0.3063, -0.3302, -0.9808,  0.1947,\n",
              "                        0.2868, -0.7308],\n",
              "                      [ 0.1748, -1.0939,  0.9633, -0.3095,  0.5712,  1.1179, -1.5469,  0.7567,\n",
              "                        0.7755,  2.0265],\n",
              "                      [ 0.9812, -0.6401, -0.4908,  0.2080, -0.9319, -1.5910, -1.1360, -0.5226,\n",
              "                        0.7165,  1.5335],\n",
              "                      [-1.4510, -0.7861,  1.0229, -0.5558,  0.7043,  0.7099, -1.5326, -0.7251,\n",
              "                        0.4664,  0.6667]])),\n",
              "             ('weights.3',\n",
              "              tensor([[ 1.3032,  0.4879,  1.1340],\n",
              "                      [-0.3556,  0.3618,  1.9993],\n",
              "                      [ 0.6630,  0.7047,  0.0213],\n",
              "                      [-0.8293, -1.0809, -0.7839],\n",
              "                      [ 0.5071,  0.0821, -0.3532],\n",
              "                      [ 1.4639,  0.1729,  1.0514],\n",
              "                      [ 0.0075, -0.0774,  0.6427],\n",
              "                      [ 0.5742,  0.5058,  0.2225],\n",
              "                      [-0.9143,  1.4840, -0.9109],\n",
              "                      [-0.5291, -0.8051,  0.5158]])),\n",
              "             ('bias.0',\n",
              "              tensor([ 1.3525,  0.6863, -0.3278,  0.7950,  0.2815,  0.0562])),\n",
              "             ('bias.1',\n",
              "              tensor([-0.2858, -1.0935,  1.1351,  0.7592, -3.5945,  0.0192,  0.1052,  0.9603])),\n",
              "             ('bias.2',\n",
              "              tensor([ 2.3839,  0.9157, -0.6430,  0.7113,  0.4000, -1.2039, -0.4198, -1.1929,\n",
              "                      -0.9351,  0.2138])),\n",
              "             ('bias.3', tensor([ 0.8228, -0.8046, -0.3734]))])"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and testing the network"
      ],
      "metadata": {
        "id": "kAV_vFT07kAz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now need to define a Loss function and a optimizer.\n",
        "\n",
        "\n",
        "\n",
        "*   Loss function: cross-entropy.\n",
        "*   Optimizer: `torch.optim.SGD`\n",
        "\n",
        "We will train the network with a `lr = 0.01` and for 200 epochs.\n",
        "We will also compute the network's accuracy."
      ],
      "metadata": {
        "id": "blwvr9-P7sIP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the loss function\n",
        "loss_f = nn.CrossEntropyLoss() # The loss already executes the softmax\n",
        "# Define the optimizer\n",
        "optimizer = torch.optim.SGD(params=model0.parameters(),\n",
        "                          lr=0.01)"
      ],
      "metadata": {
        "id": "agN-VxWh7r5u"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "epochs = 200\n",
        "\n",
        "# Empty lists to track values\n",
        "train_loss_values = []\n",
        "test_loss_values = []\n",
        "epoch_count = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    ### Training\n",
        "    model0.train()\n",
        "    train_loss = 0\n",
        "    train_acc = 0\n",
        "    train_total = 0\n",
        "\n",
        "    # Loop through training batches\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        # 1. Forward pass\n",
        "        y_pred = model0(X_batch)\n",
        "\n",
        "        # 2. Calculate loss\n",
        "        loss = loss_f(y_pred, y_batch)\n",
        "\n",
        "        # 3. Zero gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 4. Backpropagation\n",
        "        loss.backward()\n",
        "\n",
        "        # 5. Update parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate batch loss\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # Calculate the softmax over the predictions in order to compute accuracy\n",
        "        predicted = y_pred.argmax(dim=1)\n",
        "        train_acc += (predicted == y_batch).sum().item()\n",
        "        train_total += y_batch.size(0)\n",
        "\n",
        "    # Calculate average training loss for the epoch\n",
        "    train_loss = train_loss / len(train_loader)\n",
        "    train_acc /=  train_total\n",
        "\n",
        "    ### Testing\n",
        "    model0.eval()\n",
        "    test_loss = 0\n",
        "    test_acc = 0\n",
        "    test_total = 0\n",
        "    with torch.inference_mode():\n",
        "        # Loop through test batches\n",
        "        for X_batch, y_batch in test_loader:\n",
        "            # 1. Forward pass on test data\n",
        "            test_pred = model0(X_batch)\n",
        "\n",
        "            # 2. Calculate test loss\n",
        "            batch_test_loss = loss_f(test_pred, y_batch)\n",
        "\n",
        "            # Accumulate batch test loss\n",
        "            test_loss += batch_test_loss.item()\n",
        "\n",
        "            # Calculate the softmax over the predictions in order to compute accuracy\n",
        "            predicted = test_pred.argmax(dim=1)\n",
        "            test_acc += (predicted == y_batch).sum().item()\n",
        "            test_total += y_batch.size(0)\n",
        "\n",
        "        # Calculate average test loss for the epoch\n",
        "        test_loss = test_loss / len(test_loader)\n",
        "        test_acc /= test_total\n",
        "\n",
        "    # Print progress every 10 epochs\n",
        "    if epoch % 10 == 0:\n",
        "        epoch_count.append(epoch)\n",
        "        train_loss_values.append(train_loss)\n",
        "        test_loss_values.append(test_loss)\n",
        "        print(f\"Epoch: {epoch} | Train Loss: {train_loss:.5f}| Test Loss: {test_loss:.5f}\")\n",
        "        print(f\"Train acc: {train_acc} | Test acc: {test_acc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqcAwoHS7deo",
        "outputId": "06209e4a-3dd6-4c62-f6f9-c214a26ffe1b"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 | Train Loss: 0.06046| Test Loss: 0.07228\n",
            "Train acc: 0.9703703703703703 | Test acc: 0.9333333333333333\n",
            "Epoch: 10 | Train Loss: 0.06365| Test Loss: 0.01664\n",
            "Train acc: 0.9703703703703703 | Test acc: 1.0\n",
            "Epoch: 20 | Train Loss: 0.06075| Test Loss: 0.03326\n",
            "Train acc: 0.9703703703703703 | Test acc: 1.0\n",
            "Epoch: 30 | Train Loss: 0.05966| Test Loss: 0.02613\n",
            "Train acc: 0.9777777777777777 | Test acc: 1.0\n",
            "Epoch: 40 | Train Loss: 0.05768| Test Loss: 0.01355\n",
            "Train acc: 0.9777777777777777 | Test acc: 1.0\n",
            "Epoch: 50 | Train Loss: 0.08195| Test Loss: 0.03703\n",
            "Train acc: 0.9555555555555556 | Test acc: 1.0\n",
            "Epoch: 60 | Train Loss: 0.05170| Test Loss: 0.01182\n",
            "Train acc: 0.9851851851851852 | Test acc: 1.0\n",
            "Epoch: 70 | Train Loss: 0.06519| Test Loss: 0.03497\n",
            "Train acc: 0.9703703703703703 | Test acc: 1.0\n",
            "Epoch: 80 | Train Loss: 0.06684| Test Loss: 0.02449\n",
            "Train acc: 0.9629629629629629 | Test acc: 1.0\n",
            "Epoch: 90 | Train Loss: 0.05581| Test Loss: 0.02706\n",
            "Train acc: 0.9851851851851852 | Test acc: 1.0\n",
            "Epoch: 100 | Train Loss: 0.05581| Test Loss: 0.02892\n",
            "Train acc: 0.9851851851851852 | Test acc: 1.0\n",
            "Epoch: 110 | Train Loss: 0.08453| Test Loss: 0.01415\n",
            "Train acc: 0.9777777777777777 | Test acc: 1.0\n",
            "Epoch: 120 | Train Loss: 0.05557| Test Loss: 0.02259\n",
            "Train acc: 0.9703703703703703 | Test acc: 1.0\n",
            "Epoch: 130 | Train Loss: 0.08466| Test Loss: 0.01896\n",
            "Train acc: 0.9629629629629629 | Test acc: 1.0\n",
            "Epoch: 140 | Train Loss: 0.06213| Test Loss: 0.05309\n",
            "Train acc: 0.9703703703703703 | Test acc: 0.9333333333333333\n",
            "Epoch: 150 | Train Loss: 0.05186| Test Loss: 0.02056\n",
            "Train acc: 0.9777777777777777 | Test acc: 1.0\n",
            "Epoch: 160 | Train Loss: 0.07260| Test Loss: 0.01769\n",
            "Train acc: 0.9629629629629629 | Test acc: 1.0\n",
            "Epoch: 170 | Train Loss: 0.05541| Test Loss: 0.02414\n",
            "Train acc: 0.9851851851851852 | Test acc: 1.0\n",
            "Epoch: 180 | Train Loss: 0.05414| Test Loss: 0.02713\n",
            "Train acc: 0.9777777777777777 | Test acc: 1.0\n",
            "Epoch: 190 | Train Loss: 0.07588| Test Loss: 0.02666\n",
            "Train acc: 0.9777777777777777 | Test acc: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see a great performance of the network for this arquitecture and this classifying problem. Although, we suspect the test accuracy is so high because we do not have enough data."
      ],
      "metadata": {
        "id": "XwVNqVEuhmCZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S4qT_-qchl11"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}